{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_S-pHAJvfVO"
   },
   "source": [
    "1. Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "4. (опциональное, если очень хочется) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HNBRBHrvfU_"
   },
   "source": [
    "## Сначала повторим действия с урока"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wjGSHSo9wlWW",
    "outputId": "317a8150-4a14-4ca6-abf2-f1e691eb7b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2ylzczivfVA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxDIgsbdvfVA"
   },
   "source": [
    "Наши новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "cpdS6MxpvfVA",
    "outputId": "e1d0d3fd-6b7e-4dc4-a17e-535eecb630b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  Заместитель председателяnправительства РФnСерг...\n",
       "1    4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2    4897  Форвард «Авангарда» Томаш Заборский прокоммент..."
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/machine_learning_in_business/data/articles.csv\")\n",
    "print(news.shape)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPhLpoJHvfVB"
   },
   "source": [
    "Загрузим пользователей и списки последних прочитанных новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "xzUQI8NpvfVB",
    "outputId": "898bb65c-557e-4851-9250-7cea1a850608"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/machine_learning_in_business/data/users_articles.csv\")\n",
    "users.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JM0aa89tvfVB"
   },
   "source": [
    "Итак, нам нужно получить векторные представления пользователей на основе прочитанным ими новостей и самих новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XXIezIIvfVC"
   },
   "source": [
    "### 1. Получаем векторные представления новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed0LtySExxi4",
    "outputId": "22e136ec-4a90-4067-a210-89fba307b9df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (4.2.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HF737HwCvfVC"
   },
   "outputs": [],
   "source": [
    "#from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DJO6WYHe8ys3",
    "outputId": "e957a2a7-bb6d-437e-d55f-a84435dba988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: razdel in /usr/local/lib/python3.7/dist-packages (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade razdel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZ0M2Yt_8rDt",
    "outputId": "7df35f0b-0f8c-4040-ca35-917465bd802b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: nltk in /usr/local/lib/python3.7/dist-packages (3.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS9mNxI1yHC1",
    "outputId": "5f17eaa3-aaad-4acc-f0bb-ae41fc1f5640"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pymorphy2) (0.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYqNt-RTvfVC"
   },
   "outputs": [],
   "source": [
    "#предобработка текстов\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNnsbGytA-2Q",
    "outputId": "f10ce680-4109-4984-d3fb-b6c0a30dc800"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Djd4hd3z_uP"
   },
   "source": [
    "Исследование по проблемам stop-листов: https://www.aclweb.org/anthology/W18-2502/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7W9WEXXCvfVD"
   },
   "outputs": [],
   "source": [
    "stopword_ru = stopwords.words('russian')  # Убирают, например слова без смысловой нагрузки (местоимения, предлоги), мат и др.\n",
    "len(stopword_ru)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()  # Lemmatisation (Лемматизация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rq3A6Cz_vfVD"
   },
   "source": [
    "Есть 2 метода решить проблему разных словоформ:\n",
    "1. Stemming (Стемминг) - грубо говоря, обрезает концовки слов (подходит для английского языка, на русском плохо)\n",
    "2. Lemmatisation (Лемматизация) - приводится словоформа на основании заранее написанных словарей (подходит для русского языка)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IeoS3M5vfVD",
    "outputId": "3ebcbc78-c704-4de6-f3cd-5cd3de06dccf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(r\"/content/drive/MyDrive/Colab Notebooks/machine_learning_in_business/data/stopwords.txt\") as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += additional_stopwords\n",
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wC0rCkimvfVN"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgwK5rqpvfVE"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст (готовый к лемматизации)\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()  # приводим к строчному виду\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens] или считать по максимальным значениям, если важнее precision. Предположу, что preds_embeddings_mean сработало хуже у \n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                # morph.parse - выдает несколько нормальных форм в порядке убывания вероятности, но в этом кейсе мы берем только самую вероятную\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLVzdavBvfVE",
    "outputId": "067497a2-a577-42f3-91f2-11616898e627",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: FutureWarning: Possible nested set at position 39\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 1.7 s, total: 27.2 s\n",
      "Wall time: 27.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMdk_FI7vfVF",
    "outputId": "a83f1a8c-8db5-47c6-f5b4-d21661593275"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 33s, sys: 798 ms, total: 4min 34s\n",
      "Wall time: 4min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVOmBpdDvfVG"
   },
   "source": [
    "А теперь в 3 строчки обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BW5GkNuivfVG"
   },
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]  # bow - bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41BnHLpEvfVH"
   },
   "source": [
    "Все просто - это словарь наших слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbhH-6ZrvfVH"
   },
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mKhTvaBpvfVI",
    "outputId": "5aa45fe0-49d4-4d9a-9da8-4322c2ce8375"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 54.2 s, sys: 26.8 s, total: 1min 21s\n",
      "Wall time: 51.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus\n",
    "# 25 тематик задаём \"руками\" экспертно, принцип как в K-Means, алгоритм пытается разбить на указанное нами число тем\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pOOL26VGvfVI"
   },
   "source": [
    "Обучили модель. Теперь 2 вопроса:\n",
    "\n",
    "1. как выглядят наши темы\n",
    "2. как получить для документа вектор значений (вероятности принадлежности каждой теме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vIipd9JevfVI",
    "outputId": "c7f1d337-86aa-4886-c041-0377cb249ac1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['форвард', 'авангард', 'томаш', 'заборский', 'прокомментировать', 'игра', 'команда', 'матч', 'чемпионат', 'кхл', 'против', 'атланта', 'nnnn', 'плохой', 'матч', 'нижний', 'новгород', 'против', 'торпедо', 'настраиваться', 'первый', 'минута', 'включиться', 'заборский', 'получиться', 'забросить', 'быстрый', 'гол', 'задать', 'хороший', 'темп', 'поединок', 'играть', 'хороший', 'сторона', 'пять', 'очко', 'выезд', 'девять', 'хороший']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 0.17856407), (2, 0.071060926), (12, 0.4253282), (24, 0.304559)]"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6LipgUcvfVI",
    "outputId": "faa32c3b-d774-46ca-a486-9bc83cd23ac0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: исследование сша учёный американский ракета санкция век\n",
      "topic_1: всё рост очень земля большой уровень россия\n",
      "topic_2: смерть препарат тело жизнь всё память болезнь\n",
      "topic_3: египет оборудование nn живой ран продолжительность потепление\n",
      "topic_4: ребёнок гражданин россия научный женщина животное исследователь\n",
      "topic_5: советский снизить продукт общество наука геном московский\n",
      "topic_6: писать испытание страдать nthe свет дом книга\n",
      "topic_7: луна рак остров китай вода nn японский\n",
      "topic_8: газ учёный исследователь космический нефть спутник новый\n",
      "topic_9: военный россия наука министерство станция новый поток\n",
      "topic_10: украина украинский экономика киев белоруссия белый россия\n",
      "topic_11: конкурс наука супруг девочка уголь премия вуз\n",
      "topic_12: космос восток планета расстояние канал мальчик небо\n",
      "topic_13: млрд рубль банк цена рынок фонд млн\n",
      "topic_14: форум nn место рейтинг мышь параметр определение\n",
      "topic_15: nn глава совет эксперимент валюта решение организм\n",
      "topic_16: доля млн университет турист тыс музей управлять\n",
      "topic_17: сотрудник статья орган уголовный лицо россия документ\n",
      "topic_18: учёный журнал клетка медведев жизнь заболевание ребёнок\n",
      "topic_19: спрос изучение солнце расчёт дыра писать способность\n",
      "topic_20: самолёт район погибнуть обнаружить пострадать полёт произойти\n",
      "topic_21: авиакомпания миссия корабль управление режим удар ракета\n",
      "topic_22: российский россия сша американский правительство проект население\n",
      "topic_23: область рынок роскосмос школа образование власть законодательство\n",
      "topic_24: мозг выяснить взрыв врач произойти проверка лечение\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))\n",
    "# Слова в ответе в порядке убывания их влияния на тему"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpGLgQ2EvfVJ"
   },
   "source": [
    "Очень неплохо - большинство тем вполне можно описать о чем они"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHwUQ9nGvfVJ"
   },
   "source": [
    "Давайте напишем функцию, которая будет нам возвращать векторное представление новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fBqrdBWvfVJ"
   },
   "outputs": [],
   "source": [
    "#text = news['title'].iloc[0]\n",
    "\n",
    "# Теперь мы хотим, на основании разбиения по темам выше, каждую статью представлять в виде вектора, который содержит для каждой темы % близости к ней\n",
    "def get_lda_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)  # из новой статьи получили слова\n",
    "    lda_tuple = lda[unseen_doc]  # Присвоили тематику\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "WKik8PUsvfVJ",
    "outputId": "db1b86ae-1f43-4ee1-f784-81f08aeb346c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059853</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.662004</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.148688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048933</td>\n",
       "      <td>0.206972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.659581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163405</td>\n",
       "      <td>0.127505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.393815</td>\n",
       "      <td>0.191307</td>\n",
       "      <td>0.112697</td>\n",
       "      <td>0.066589</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.070579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.088075</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068366</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072210</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.097137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.091605</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.716345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id  topic_0   topic_1   topic_2  ...  topic_21  topic_22  topic_23  topic_24\n",
       "0       6      0.0  0.000000  0.000000  ...  0.011215       0.0       0.0  0.000000\n",
       "1    4896      0.0  0.000000  0.000000  ...  0.000000       0.0       0.0  0.659581\n",
       "2    4897      0.0  0.163405  0.127505  ...  0.000000       0.0       0.0  0.290958\n",
       "3    4898      0.0  0.393815  0.191307  ...  0.000000       0.0       0.0  0.000000\n",
       "4    4899      0.0  0.072210  0.000000  ...  0.000000       0.0       0.0  0.000000\n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gtwI5y7WvfVJ"
   },
   "source": [
    "Прекрасно, мы получили вектора наших новостей! И даже умеем интерпретировать получившиеся темы.\n",
    "\n",
    "Можно двигаться далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUyWMrbdvfVK"
   },
   "source": [
    "### Следующий шаг - векторные представления пользователей\n",
    "\n",
    "Далее мы хотим для каждого юзера взять векторы его статей где проставлены вероятностные распределения по темам и найти среднее по каждой теме. Это и будет эмбеддинг, характеризующий юзера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "2DGnyT4xvfVK",
    "outputId": "bd60ca3f-d111-4e00-bdcc-5af584d03a7e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAasZmiyvfVK"
   },
   "outputs": [],
   "source": [
    "# topic_matrix переводим в векторную форму\n",
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2HQc8aiSLwE0"
   },
   "source": [
    "## Модифицируем функцию get_user_embedding и сравним метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "90JogShWvfVK"
   },
   "outputs": [],
   "source": [
    "def get_user_embedding_new(user_articles_list, method='mean'):\n",
    "    \"\"\"\n",
    "    method: 'mean' / 'median' / 'max'\n",
    "    \"\"\"\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    if method == 'mean':\n",
    "        user_vector = np.mean(user_vector, axis=0)  # Вот тут находим среднее по темам для юзера\n",
    "    elif method == 'median':\n",
    "        user_vector = np.median(user_vector, axis=0)\n",
    "    elif method == 'max':\n",
    "        user_vector = np.max(user_vector, 0)  # идентично axis=0\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zBzUpIMJvfVL",
    "outputId": "07ca9d01-4a45-41bc-ec36-8de43e14033a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05655881, 0.05252899, 0.00217402, 0.        , 0.07903294,\n",
       "       0.        , 0.        , 0.02688554, 0.01185732, 0.07967297,\n",
       "       0.01197489, 0.00810417, 0.        , 0.0145253 , 0.        ,\n",
       "       0.12341383, 0.        , 0.18974402, 0.        , 0.02021861,\n",
       "       0.02447238, 0.03903681, 0.2306012 , 0.0124764 , 0.00331886])"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_articles_list = users['articles'].iloc[33]\n",
    "get_user_embedding_new(user_articles_list, method='mean')\n",
    "#get_user_embedding_new(user_articles_list, method='median')\n",
    "#get_user_embedding_new(user_articles_list, method='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8f83TNn7vfVL"
   },
   "source": [
    "Теперь получим эмбединги для всех пользователей и проверим их качество на конкретной downstream-задаче"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 162
    },
    "id": "er9jg85dvfVL",
    "outputId": "d7bd52d8-85a8-461d-ec34-4165132c578e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>topic_10</th>\n",
       "      <th>topic_11</th>\n",
       "      <th>topic_12</th>\n",
       "      <th>topic_13</th>\n",
       "      <th>topic_14</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>0.032645</td>\n",
       "      <td>0.036736</td>\n",
       "      <td>0.021159</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.127754</td>\n",
       "      <td>0.054568</td>\n",
       "      <td>0.094175</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035990</td>\n",
       "      <td>0.040073</td>\n",
       "      <td>0.040095</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.047739</td>\n",
       "      <td>0.104646</td>\n",
       "      <td>0.073463</td>\n",
       "      <td>0.118035</td>\n",
       "      <td>0.018667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055156</td>\n",
       "      <td>0.015061</td>\n",
       "      <td>0.015198</td>\n",
       "      <td>0.023364</td>\n",
       "      <td>0.023328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>0.025113</td>\n",
       "      <td>0.123114</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.011190</td>\n",
       "      <td>0.074025</td>\n",
       "      <td>0.005112</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.057041</td>\n",
       "      <td>0.126383</td>\n",
       "      <td>0.017151</td>\n",
       "      <td>0.005932</td>\n",
       "      <td>0.036217</td>\n",
       "      <td>0.006879</td>\n",
       "      <td>0.053808</td>\n",
       "      <td>0.002701</td>\n",
       "      <td>0.114957</td>\n",
       "      <td>0.056452</td>\n",
       "      <td>0.022348</td>\n",
       "      <td>0.038107</td>\n",
       "      <td>0.013341</td>\n",
       "      <td>0.141529</td>\n",
       "      <td>0.022408</td>\n",
       "      <td>0.027751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.027491</td>\n",
       "      <td>0.007229</td>\n",
       "      <td>0.008685</td>\n",
       "      <td>0.035024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.054504</td>\n",
       "      <td>0.008956</td>\n",
       "      <td>0.115897</td>\n",
       "      <td>0.018095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002677</td>\n",
       "      <td>0.022503</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.080475</td>\n",
       "      <td>0.007531</td>\n",
       "      <td>0.198754</td>\n",
       "      <td>0.024490</td>\n",
       "      <td>0.013623</td>\n",
       "      <td>0.122559</td>\n",
       "      <td>0.030112</td>\n",
       "      <td>0.074670</td>\n",
       "      <td>0.051376</td>\n",
       "      <td>0.058700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid   topic_0   topic_1  ...  topic_22  topic_23  topic_24\n",
       "0  u105138  0.032645  0.036736  ...  0.015198  0.023364  0.023328\n",
       "1  u108690  0.025113  0.123114  ...  0.141529  0.022408  0.027751\n",
       "2  u108339  0.002503  0.027491  ...  0.074670  0.051376  0.058700\n",
       "\n",
       "[3 rows x 26 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings_mean = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_new(x, method='mean'), 1)])\n",
    "user_embeddings_mean.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings_mean['uid'] = users['uid'].values\n",
    "user_embeddings_mean = user_embeddings_mean[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "user_embeddings_mean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64cED-VQcqQ9"
   },
   "outputs": [],
   "source": [
    "user_embeddings_median = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_new(x, method='median'), 1)])\n",
    "user_embeddings_median.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings_median['uid'] = users['uid'].values\n",
    "user_embeddings_median = user_embeddings_median[['uid']+['topic_{}'.format(i) for i in range(25)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btsHTajucq4m"
   },
   "outputs": [],
   "source": [
    "user_embeddings_max = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding_new(x, method='max'), 1)])\n",
    "user_embeddings_max.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings_max['uid'] = users['uid'].values\n",
    "user_embeddings_max = user_embeddings_max[['uid']+['topic_{}'.format(i) for i in range(25)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgdiYrZdvfVM"
   },
   "source": [
    "Датасет готов - можно попробовать обучить модель. Загрузим нашу разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "81ObxOV0vfVM",
    "outputId": "a3efd845-fa31-42b0-ab74-bcabb4f8b8fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u107120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u102277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u102444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid  churn\n",
       "0  u107120      0\n",
       "1  u102277      0\n",
       "2  u102444      0"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = pd.read_csv(r\"/content/drive/MyDrive/Colab Notebooks/machine_learning_in_business/data/users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9WF18HNdCS2"
   },
   "outputs": [],
   "source": [
    "def make_predictions(user_embeddings, target):\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                    X['churn'], random_state=0)\n",
    "    logreg = LogisticRegression()\n",
    "    #обучим\n",
    "    logreg.fit(X_train, y_train)\n",
    "    #наши прогнозы для тестовой выборки\n",
    "    preds = logreg.predict_proba(X_test)[:, 1]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7DZh7nCjeiMo",
    "outputId": "c2c3400c-d085-4491-949e-b438d64f6443"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2138205 , 0.02330607, 0.3836392 , 0.42299591, 0.04971261,\n",
       "       0.03834308, 0.15292561, 0.05150341, 0.05278461, 0.13061209])"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_embeddings_mean = make_predictions(user_embeddings_mean, target)\n",
    "preds_embeddings_median = make_predictions(user_embeddings_median, target)\n",
    "preds_embeddings_max = make_predictions(user_embeddings_max, target)\n",
    "preds_embeddings_mean[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KXnrGCK8vfVN"
   },
   "source": [
    "### Рассчитаем Precision, Recall, F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rw8NYoKPnSwh"
   },
   "outputs": [],
   "source": [
    "def metrics_calculation(y_test, preds):\n",
    "\n",
    "    roc_auc = roc_auc_score(y_test, preds)\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    # locate the index of the largest f score\n",
    "    ix = np.argmax(fscore)\n",
    "    return thresholds[ix], fscore[ix], precision[ix], recall[ix], roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44QYc4vepOyh"
   },
   "outputs": [],
   "source": [
    "all_preds = [preds_embeddings_mean, preds_embeddings_median, preds_embeddings_max]\n",
    "metrics = []\n",
    "for preds in all_preds:\n",
    "    best_threshold, fscore, precision, recall, roc_auc = metrics_calculation(y_test, preds)\n",
    "    metrics.append([fscore, precision, recall, roc_auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "id": "2s0GMKQdpONP",
    "outputId": "e89f990a-7c09-4462-a239-3aded0f167b3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fscore</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>preds_embeddings_mean</th>\n",
       "      <td>0.692857</td>\n",
       "      <td>0.615873</td>\n",
       "      <td>0.791837</td>\n",
       "      <td>0.950095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds_embeddings_median</th>\n",
       "      <td>0.757282</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.967580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>preds_embeddings_max</th>\n",
       "      <td>0.762475</td>\n",
       "      <td>0.746094</td>\n",
       "      <td>0.779592</td>\n",
       "      <td>0.970698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           fscore  precision    recall   roc_auc\n",
       "preds_embeddings_mean    0.692857   0.615873  0.791837  0.950095\n",
       "preds_embeddings_median  0.757282   0.722222  0.795918  0.967580\n",
       "preds_embeddings_max     0.762475   0.746094  0.779592  0.970698"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_comparison_df = pd.DataFrame(np.array(metrics),\n",
    "                   columns=['fscore', 'precision', 'recall', 'roc_auc'],\n",
    "                   index=['preds_embeddings_mean', 'preds_embeddings_median', 'preds_embeddings_max'])\n",
    "metrics_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7lE2WzWvfVO"
   },
   "source": [
    "В данном кейсе можно останавливаться на эмбеддингах по медиане, если для нас важнее recall или считать по максимальным значениям, если важнее precision. Предположу, что preds_embeddings_mean сработало хуже у меня, т.к. я на применял TF-IDF и выбросы испортили результат, т.к. среднее чувствительно к выбросам."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1HNBRBHrvfU_"
   ],
   "name": "hw_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
